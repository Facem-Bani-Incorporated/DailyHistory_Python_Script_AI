import asyncio
import os
import httpx
from datetime import datetime
from sqlalchemy import text
from core.logger import setup_logger
from core.config import config
# ImportƒÉm »ôi noul model ProcessedEvent pentru arhivare
from core.database import engine, AsyncSessionLocal, IngestionLog, ProcessedEvent
from engine.scraper import WikiScraper
from engine.processor import AIProcessor
from engine.ranker import ScoringEngine
from schema.models import DailyPayload, MainEvent, SecondaryEvent
from tenacity import retry, stop_after_attempt, wait_fixed

logger = setup_logger("MainPipeline")


# --- FUNC»öIA DE SALVARE LOGURI (AUDIT) ---
async def save_event_content(payload: DailyPayload):
    """SalveazƒÉ con»õinutul evenimentului principal √Æn tabelul processed_events."""
    if engine is None: return

    try:
        async with AsyncSessionLocal() as session:
            async with session.begin():
                main = payload.main_event

                # TransformƒÉm manual obiectele √Æn dic»õionare simple
                # Folosim dict() pentru a fi siguri cƒÉ eliminƒÉm clasa Translations
                titles_dict = dict(main.title_translations)
                narrative_dict = dict(main.narrative_translations)

                new_entry = ProcessedEvent(
                    event_date=payload.date_processed,
                    year=main.year,
                    titles=titles_dict,
                    narrative=narrative_dict,
                    image_url=main.gallery[0] if main.gallery else None,
                    impact_score=main.impact_score,
                    source_url=main.source_url
                )
                session.add(new_entry)
            await session.commit()
        logger.info(f"üèõÔ∏è Con»õinutul evenimentului din {main.year} a fost ARHIVAT √Æn DB.")
    except Exception as e:
        logger.error(f"‚ùå Eroare la arhivarea con»õinutului: {e}")
        # Foarte important: ridicƒÉm eroarea mai departe pentru a fi prinsƒÉ de blocul general
        raise


# --- NOUA FUNC»öIE DE ARHIVARE CON»öINUT (DATE REALE) ---
async def save_event_content(payload: DailyPayload):
    """SalveazƒÉ efectiv textele traduse »ôi link-urile pozelor √Æn DB."""
    if engine is None: return

    try:
        async with AsyncSessionLocal() as session:
            async with session.begin():
                main = payload.main_event
                new_entry = ProcessedEvent(
                    event_date=payload.date_processed,
                    year=main.year,
                    titles=main.title_translations,
                    narrative=main.narrative_translations,
                    image_url=main.gallery[0] if main.gallery else None,
                    impact_score=main.impact_score,
                    source_url=main.source_url
                )
                session.add(new_entry)
            await session.commit()
        logger.info(f"üèõÔ∏è Con»õinutul evenimentului din {main.year} a fost ARHIVAT √Æn DB.")
    except Exception as e:
        logger.error(f"‚ùå Eroare la arhivarea con»õinutului: {e}")


# --- FUNC»öIA DE TRANSMISIE CƒÇTRE JAVA (COMENTATƒÇ √éN MAIN) ---
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
async def send_to_java(payload: DailyPayload):
    headers = {
        "X-Internal-Api-Key": config.INTERNAL_API_SECRET,
        "Content-Type": "application/json"
    }
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.post(
            config.JAVA_BACKEND_URL,
            json=payload.model_dump(mode='json'),
            headers=headers
        )
        response.raise_for_status()
        return response.status_code


# --- PIPELINE-UL PRINCIPAL ---
async def main():
    logger.info("üöÄ Pornire Pipeline cu Arhivare LocalƒÉ...")

    # 0. Sincronizare Tabele
    try:
        from core.database import init_db
        await init_db()
    except Exception as e:
        logger.error(f"‚ùå Eroare init DB: {e}")

    current_main_year = None
    current_score = None

    try:
        # 1. SETUP MODULE
        scraper = WikiScraper()
        processor = AIProcessor()
        ranker = ScoringEngine()
        logger.info("‚öôÔ∏è Module ini»õializate.")

        # 2. FETCH & RANK
        raw_events = await scraper.fetch_today()
        if not raw_events:
            raise ValueError("Nu s-au gƒÉsit evenimente pe Wikipedia.")

        for item in raw_events:
            item['h_score'] = ranker.heuristic_score(item)

        candidates = sorted(raw_events, key=lambda x: x['h_score'], reverse=True)[:50]
        logger.info(f"üìö Preluat {len(candidates)} candida»õi.")

        # 3. AI SCORING & TRANSLATION
        ai_data = await processor.batch_score_and_translate_titles(candidates)
        for idx, item in enumerate(candidates):
            res = ai_data['results'].get(f"ID_{idx}", {"score": 50, "titles": {}})
            item['final_score'] = ranker.hybrid_calculate(item['h_score'], res['score'])
            item['titles'] = res['titles']

        candidates.sort(key=lambda x: x['final_score'], reverse=True)
        top_data = candidates[0]
        current_main_year = top_data['year']
        current_score = top_data['final_score']
        logger.info(f"ü§ñ AI Scoring gata pentru anul {current_main_year}.")

        # 4. MEDIA CONTENT (CLOUDINARY)
        main_content = await processor.generate_multilingual_main_event(top_data['text'], top_data['year'])
        p_main = top_data.get("pages", [])
        slug_main = p_main[0].get("titles", {}).get("canonical") if p_main else "history"

        wiki_imgs = await scraper.fetch_gallery_urls(slug_main, limit=3)
        main_gallery = [scraper.upload_to_cloudinary(url, f"main_{top_data['year']}_{i}") for i, url in
                        enumerate(wiki_imgs)]
        logger.info("üñºÔ∏è Media urcatƒÉ pe Cloudinary.")

        # 5. SECONDARY EVENTS & PAYLOAD
        secondary_objs = []
        for idx, item in enumerate(candidates[1:6]):
            p_sec = item.get("pages", [])
            slug_sec = p_sec[0].get("titles", {}).get("canonical") if p_sec else ""
            thumb = None
            if slug_sec:
                imgs_sec = await scraper.fetch_gallery_urls(slug_sec, limit=1)
                if imgs_sec:
                    thumb = scraper.upload_to_cloudinary(imgs_sec[0], f"sec_{item['year']}_{idx}")

            secondary_objs.append(SecondaryEvent(
                title_translations=item['titles'],
                year=item['year'],
                source_url=f"https://en.wikipedia.org/wiki/{slug_sec}",
                ai_relevance_score=item['final_score'],
                thumbnail_url=thumb
            ))

        payload = DailyPayload(
            date_processed=datetime.now().date(),
            api_secret=config.INTERNAL_API_SECRET,
            main_event=MainEvent(
                title_translations=main_content['titles'],
                year=top_data['year'],
                source_url=f"https://en.wikipedia.org/wiki/{slug_main}",
                event_date=datetime.now().date(),
                narrative_translations=main_content['narratives'],
                impact_score=top_data['final_score'],
                gallery=[img for img in main_gallery if img]
            ),
            secondary_events=secondary_objs
        )

        # --- SALVARE DATE ---
        # SalvƒÉm con»õinutul complet √Æn baza de date localƒÉ
        await save_event_content(payload)

        # Java Bridge - COMENTAT
        # await send_to_java(payload)
        # logger.info("‚úÖ Trimis cƒÉtre Java!")

        logger.info("‚ö†Ô∏è Java Bridge ignorat (Simulare). Datele sunt √Æn DB.")
        await log_to_db(status="SUCCESS", year=current_main_year, score=current_score)
        logger.info("‚ú® Pipeline finalizat cu succes!")

    except Exception as e:
        error_msg = str(e)
        logger.error(f"üö® Pipeline Crash: {error_msg}")
        await log_to_db(status="ERROR", year=current_main_year, error=error_msg)

    finally:
        if engine:
            await engine.dispose()
            logger.info("üîå Conexiune DB √ÆnchisƒÉ.")


if __name__ == "__main__":
    asyncio.run(main())